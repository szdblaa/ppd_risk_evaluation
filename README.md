背景介绍
==
比赛公开了国内网络借贷行业的贷款风险数据

  1、包括信用违约标签（因变量）
  
  2、建模所需的基础与加⼯字段（自变量）

  3、相关用户的网络行为原始数据

本着保护借款⼈隐私以及拍拍贷知识产权的目的，数据字段已经过脱敏处理。

数据简介
==
  1、数据编码为GBK。
  
  2、初赛数据包括3万条训练集和2万条测试集。
  
  3、复赛会增加新的3万条数据，供参赛团队优化模型，并新增1万条数据作为测试集。
  
  所有训练集，测试集都包括3个csv文件。
  
数据信息
==
Master(每一行代表一个成功成交借款样本，每个样本包含200多个各类字段)
--

  idx：每笔贷款的unique key，可与另外2个文件里的idx相匹配。
  
  UserInfo_*：借款人特征字段
  
  WeblogInfo_*：Info网络行为字段
  
  Education_Info*：学历学籍字段
  
  ThirdParty_Info_PeriodN_*：第三方数据时间段N字段
  
  SocialNetwork_*：社交网络字段
  
  LinstingInfo：借款成交时间
  
  Target：违约标签（1 = 贷款违约，0 = 正常还款）。测试集里不包含target字段。
  
Log_Info（借款人的登陆信息）
--
  ListingInfo：借款成交时间
  
  LogInfo1：操作代码
  
  LogInfo2：操作类别
  
  LogInfo3：登陆时间
  
  idx：每一笔贷款的unique key
  
Userupdate_Info（借款⼈修改信息）
--
  ListingInfo1：借款成交时间
  
  UserupdateInfo1：修改内容
  
  UserupdateInfo2：修改时间
  
  idx：每⼀笔贷款的unique key
  
案例分析思路
==
1.数据清洗 对缺失值的多维度的处理 剔除常变量 文本处理 

2.特征工程 地理位置信息处理 成交时间 类别编码 组合特征 

3.特征选择 Xgboost重要度排序 

4.建模与调优 

第一部分数据清洗
==
缺失值的多维度处理；
--
对于每一列来说，统计每一列的数据缺失比例，剔除缺失率大于99%的特征

对于每一行来说，先利用Xgboost在原始的训练集上训练数据得出每一个原始特征的重要程度，再统计每个样本在TOP20重要的特征中缺失的特征数，如果其缺失的原始特征中达到10个，那就认为这个样本为异常值，应该剔除掉

剔除常变量
--
原始数据中有190维数值型特征，剔除掉标准差小于0.1的特征项

其余的文本处理
--
  1.字符大小写转换 Userupdate_Info表中的UserupdateInfo1字段，属性取值为英文字符，包含了大小写，如“_QQ”和“_qQ”，很明显是同一种取值，我们将所有字符统一转换为小写。 

  2.空格符号处理 Master 表中 UserInfo_9字段的取值包含了空格字符，如“中国移动”和“中国移动 ”，它们是同一种取值，需要将空格符去除。 

  3.城市名处理 UserInfo_8 包含有“重庆”、“重庆市”等取值，它们实际上是同一个城市，需要把字符中的“市”全部去掉。去掉“市”之后，城市数由 600 多下降到 400 多。

第二部分特征工程
==
1.（省份）地理位置的处理方法 
--
UserInfo_7和UserInfo_19是省份信息，由于省份类别不多，故对UserInfo_19进行独热编码

2.（市级）地理位置处理 
--
UserInfo_2、UserInfo_4、UserInfo_8、UserInfo_20为城市信息。由于市级城市数量过多，如果按照类别型特征直接处理，进行独热编码后，会得到很高的维度的稀疏特征，这样训练的时候，每一维度的城市特征是学不到什么有用的权重的。故不可采取这种办法，这里我们引入城市经济等级和经纬度特征来替换原始特征

3.构建地理位置的组合特征
--
构造衡量两个地址特征是否相同的特征,例如UserInfo_2,UserInfo_4,UserInfo_8,UserInfo_20,都是城市地理信息，我们可以两两比较，构造diff_24,当UserInfo_2，UserInfo_4这两个特征值一样时，diff_24为1，否则为0，依次类推，可以构造类似diff_28,diff_220.......等特征。

4.成交时间
--
用时间特征中的"Day", "Year", "DayofYear", "DayofMonth", "DayofWeek"替代原始特征

5.类别型特征
--
  1.首先对数值型特征进行取值个数统计,将取值个数少于10个的作为类型特征处理
  
  2.类别特征中,将类别占比小于1%的类别归为一类,以减少类别数,再进行独热编码
  
6.第三方数据特征
--
由之前的特征重要性排序我们知道,ThirdParty系列总119维数据非常重要,这里我们单独处理。

根据分析我们得出,ThirdParty系列数据是17个第三方数据指标在7天内的表现数据,是与时序相关的特征。

所以我们首先构造"median", "std", "min", "max", "first","mean"的统计指标,然后构造了捕捉时间序列上的变化及趋势特征,比如

third_party_info1_1_mean表示在peiod1的值除以period1~7的均值,这是为了捕捉这个变量是否在最近发生了大的变化。
  
7.UpadteInfo表特征
--
根据这个表提供的信息，我们可以从中抽取用户修改信息次数，修改信息时间到成交时间的跨度,每种信息的修改次数等等特征

8.LogInfo表特征
--
类似的从登录信息表里提取了用户的登录信息特征,比如登录天数,平均登录间隔,以及每一种操作代码次数等等特征。

第三部分特征选择
==
Pearson相关检验,剔除相关性系数大于0.99的特征.当然也可以再次利用xgboost输出特征的重要性,据此可以保留 TopN个特征，从而达到特征选择的目的。


第四部分建模与调参
==
Logistic regression + L1正则化

randomforest

Xgboost 

调参主要针对Xgboost进行：
  
  1.首先利用xgb.cv获得最优n_estimators
  
  2.利用GridSearchCV调整max_depth和min_child_weight
